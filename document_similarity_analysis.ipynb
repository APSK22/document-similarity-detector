{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbb0dab",
   "metadata": {},
   "source": [
    "# Document Similarity Detector - Analysis Notebook\n",
    "\n",
    "This notebook demonstrates the implementation and analysis of various document similarity metrics used in Information Retrieval and Semantic Web applications.\n",
    "\n",
    "## Topics Covered:\n",
    "- **Vector Space Models** (TF-IDF)\n",
    "- **Cosine Similarity**\n",
    "- **Jaccard Index**\n",
    "- **Latent Semantic Analysis (LSA)**\n",
    "- **BERT Embeddings**\n",
    "- **IR Evaluation Metrics** (Precision, Recall, F1-Score)\n",
    "- **Embedding Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source venv/bin/activate\n",
    "pip install -r requirements-notebook.txt  # First time only\n",
    "jupyter notebook document_similarity_analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3d50f6",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bfbd4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import re\n",
    "from collections import Counter\n",
    "from IPython.display import HTML, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('dark_background')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bcaaf7",
   "metadata": {},
   "source": [
    "## 2. Sample Documents\n",
    "\n",
    "Let's load our sample documents for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9818c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "--------------------------------------------------------------------------------\n",
      "# Sample Document 1\n",
      "\n",
      "This is a sample text document about artificial intelligence and machine learning.\n",
      "Artificial intelligence has revolutionized many industries including healthcare, finance, and transportation.\n",
      "\n",
      "Machine learning algorithms can analyze vast amounts of data and identify patterns that humans might miss.\n",
      "Deep learning, a subset of machine learning, uses neural networks to process information in layers.\n",
      "\n",
      "Natural language processing enables computers to understand and generate human language.\n",
      "Computer vision allows machines to interpret and understand visual information from the world.\n",
      "\n",
      "The future of AI includes advancements in robotics, autonomous systems, and personalized medicine.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Document 2:\n",
      "--------------------------------------------------------------------------------\n",
      "# Sample Document 2\n",
      "\n",
      "This text discusses machine learning and artificial intelligence applications.\n",
      "Machine learning has transformed healthcare, finance, and many other sectors.\n",
      "\n",
      "AI algorithms process large datasets to find patterns and make predictions.\n",
      "Neural networks and deep learning architectures enable advanced data processing.\n",
      "\n",
      "NLP technology helps computers comprehend and produce human language.\n",
      "Image recognition through computer vision lets machines understand visual data.\n",
      "\n",
      "Future developments in AI will impact robotics, automation, and healthcare personalization.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load sample documents\n",
    "with open('sample_doc1.txt', 'r') as f:\n",
    "    doc1 = f.read()\n",
    "\n",
    "with open('sample_doc2.txt', 'r') as f:\n",
    "    doc2 = f.read()\n",
    "\n",
    "print(\"Document 1:\")\n",
    "print(\"-\" * 80)\n",
    "print(doc1)\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "print(\"Document 2:\")\n",
    "print(\"-\" * 80)\n",
    "print(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d60863d",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing\n",
    "\n",
    "Clean and preprocess the documents for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c3cc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: 94 words, 70 unique words\n",
      "Document 2: 73 words, 62 unique words\n",
      "Common words: 34\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Preprocess documents\n",
    "doc1_clean = preprocess_text(doc1)\n",
    "doc2_clean = preprocess_text(doc2)\n",
    "\n",
    "# Get word statistics\n",
    "words1 = doc1_clean.split()\n",
    "words2 = doc2_clean.split()\n",
    "\n",
    "print(f\"Document 1: {len(words1)} words, {len(set(words1))} unique words\")\n",
    "print(f\"Document 2: {len(words2)} words, {len(set(words2))} unique words\")\n",
    "print(f\"Common words: {len(set(words1).intersection(set(words2)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f54785",
   "metadata": {},
   "source": [
    "## 4. Similarity Metrics Implementation\n",
    "\n",
    "### 4.1 Jaccard Index (Set-Based Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe8d969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1 size: 70\n",
      "Set 2 size: 62\n",
      "Intersection: 34\n",
      "Union: 98\n",
      "Jaccard Index: 0.3469\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(text1, text2):\n",
    "    \"\"\"Calculate Jaccard similarity coefficient.\"\"\"\n",
    "    words1 = set(preprocess_text(text1).split())\n",
    "    words2 = set(preprocess_text(text2).split())\n",
    "    \n",
    "    intersection = words1.intersection(words2)\n",
    "    union = words1.union(words2)\n",
    "    \n",
    "    jaccard = len(intersection) / len(union) if union else 0.0\n",
    "    \n",
    "    print(f\"Set 1 size: {len(words1)}\")\n",
    "    print(f\"Set 2 size: {len(words2)}\")\n",
    "    print(f\"Intersection: {len(intersection)}\")\n",
    "    print(f\"Union: {len(union)}\")\n",
    "    print(f\"Jaccard Index: {jaccard:.4f}\")\n",
    "    \n",
    "    return jaccard\n",
    "\n",
    "jaccard_score = jaccard_similarity(doc1, doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034360f",
   "metadata": {},
   "source": [
    "### 4.2 Cosine Similarity with TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23ce7f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (2, 95)\n",
      "Vocabulary Size: 95\n",
      "Cosine Similarity: 0.5534\n",
      "\n",
      "Vector dimensions: 95\n",
      "Non-zero elements in Doc1: 68\n",
      "Non-zero elements in Doc2: 61\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([doc1, doc2])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "\n",
    "print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Vocabulary Size: {len(vectorizer.vocabulary_)}\")\n",
    "print(f\"Cosine Similarity: {cosine_sim:.4f}\")\n",
    "\n",
    "# Get TF-IDF vectors for collapsible display\n",
    "tfidf_doc1 = tfidf_matrix[0].toarray()[0]\n",
    "tfidf_doc2 = tfidf_matrix[1].toarray()[0]\n",
    "\n",
    "print(f\"\\nVector dimensions: {len(tfidf_doc1)}\")\n",
    "print(f\"Non-zero elements in Doc1: {np.count_nonzero(tfidf_doc1)}\")\n",
    "print(f\"Non-zero elements in Doc2: {np.count_nonzero(tfidf_doc2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077be812",
   "metadata": {},
   "source": [
    "#### Display TF-IDF Vectors in Collapsible Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "328f5252",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Display TF-IDF vectors\u001b[39;00m\n\u001b[32m     58\u001b[39m feature_names = vectorizer.get_feature_names_out()\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m html_doc1 = \u001b[43mcreate_collapsible_vector_display\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_doc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43müìÑ Document 1 TF-IDF Vector\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m html_doc2 = create_collapsible_vector_display(tfidf_doc2, \u001b[33m\"\u001b[39m\u001b[33müìÑ Document 2 TF-IDF Vector\u001b[39m\u001b[33m\"\u001b[39m, feature_names)\n\u001b[32m     62\u001b[39m display(HTML(html_doc1))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mcreate_collapsible_vector_display\u001b[39m\u001b[34m(vector, title, feature_names)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create an HTML collapsible display for vectors.\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Get top features\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_names:\n\u001b[32m      6\u001b[39m     top_indices = np.argsort(vector)[::-\u001b[32m1\u001b[39m][:\u001b[32m20\u001b[39m]\n\u001b[32m      7\u001b[39m     vector_data = [(feature_names[i], vector[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m top_indices \u001b[38;5;28;01mif\u001b[39;00m vector[i] > \u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "def create_collapsible_vector_display(vector, title, feature_names=None):\n",
    "    \"\"\"Create an HTML collapsible display for vectors.\"\"\"\n",
    "    \n",
    "    # Get top features\n",
    "    if feature_names:\n",
    "        top_indices = np.argsort(vector)[::-1][:20]\n",
    "        vector_data = [(feature_names[i], vector[i]) for i in top_indices if vector[i] > 0]\n",
    "    else:\n",
    "        vector_data = [(f\"Dim {i}\", val) for i, val in enumerate(vector) if val > 0][:20]\n",
    "    \n",
    "    # Create HTML table\n",
    "    rows = \"\"\n",
    "    for feature, value in vector_data:\n",
    "        bar_width = int(value * 100) if value <= 1 else int((value / max([v for _, v in vector_data])) * 100)\n",
    "        rows += f\"\"\"\n",
    "        <tr>\n",
    "            <td style=\"padding: 8px; color: #e6edf3;\">{feature}</td>\n",
    "            <td style=\"padding: 8px; color: #58a6ff; font-family: monospace;\">{value:.4f}</td>\n",
    "            <td style=\"padding: 8px;\">\n",
    "                <div style=\"background: #1f2937; border-radius: 4px; overflow: hidden;\">\n",
    "                    <div style=\"background: linear-gradient(90deg, #58a6ff, #3fb950); \n",
    "                                height: 20px; width: {bar_width}%;\"></div>\n",
    "                </div>\n",
    "            </td>\n",
    "        </tr>\n",
    "        \"\"\"\n",
    "    \n",
    "    html = f\"\"\"\n",
    "    <details style=\"background: #161b22; border: 1px solid #30363d; \n",
    "                    border-radius: 8px; padding: 16px; margin: 10px 0;\">\n",
    "        <summary style=\"cursor: pointer; font-size: 18px; font-weight: bold; \n",
    "                        color: #58a6ff; user-select: none;\">\n",
    "            {title} (Click to expand/collapse)\n",
    "        </summary>\n",
    "        <div style=\"margin-top: 16px;\">\n",
    "            <table style=\"width: 100%; border-collapse: collapse; color: #e6edf3;\">\n",
    "                <thead>\n",
    "                    <tr style=\"background: #0d1117; border-bottom: 2px solid #30363d;\">\n",
    "                        <th style=\"padding: 10px; text-align: left;\">Feature</th>\n",
    "                        <th style=\"padding: 10px; text-align: left;\">TF-IDF Score</th>\n",
    "                        <th style=\"padding: 10px; text-align: left;\">Weight</th>\n",
    "                    </tr>\n",
    "                </thead>\n",
    "                <tbody>\n",
    "                    {rows}\n",
    "                </tbody>\n",
    "            </table>\n",
    "            <p style=\"margin-top: 16px; color: #8b949e; font-size: 14px;\">\n",
    "                Showing top {len(vector_data)} features out of {np.count_nonzero(vector)} non-zero elements\n",
    "            </p>\n",
    "        </div>\n",
    "    </details>\n",
    "    \"\"\"\n",
    "    \n",
    "    return html\n",
    "\n",
    "# Display TF-IDF vectors\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "html_doc1 = create_collapsible_vector_display(tfidf_doc1, \"üìÑ Document 1 TF-IDF Vector\", feature_names)\n",
    "html_doc2 = create_collapsible_vector_display(tfidf_doc2, \"üìÑ Document 2 TF-IDF Vector\", feature_names)\n",
    "\n",
    "display(HTML(html_doc1))\n",
    "display(HTML(html_doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12405d2e",
   "metadata": {},
   "source": [
    "### 4.3 Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ac2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LSA using Truncated SVD\n",
    "n_components = min(50, tfidf_matrix.shape[1] - 1)\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "lsa_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Calculate similarity in LSA space\n",
    "lsa_sim = cosine_similarity(lsa_matrix[0:1], lsa_matrix[1:2])[0][0]\n",
    "\n",
    "print(f\"Original dimensions: {tfidf_matrix.shape[1]}\")\n",
    "print(f\"Reduced dimensions: {n_components}\")\n",
    "print(f\"Explained variance ratio: {svd.explained_variance_ratio_.sum():.4f}\")\n",
    "print(f\"LSA Similarity: {lsa_sim:.4f}\")\n",
    "\n",
    "# Get LSA vectors\n",
    "lsa_doc1 = lsa_matrix[0]\n",
    "lsa_doc2 = lsa_matrix[1]\n",
    "\n",
    "# Display LSA vectors in collapsible format\n",
    "html_lsa1 = create_collapsible_vector_display(lsa_doc1, \"üîç Document 1 LSA Vector\")\n",
    "html_lsa2 = create_collapsible_vector_display(lsa_doc2, \"üîç Document 2 LSA Vector\")\n",
    "\n",
    "display(HTML(html_lsa1))\n",
    "display(HTML(html_lsa2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bac548",
   "metadata": {},
   "source": [
    "### 4.4 BERT Embeddings (Contextual Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa0f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model\n",
    "print(\"Loading BERT model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"‚úì Model loaded!\")\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    \"\"\"Get BERT embeddings for text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Use mean pooling\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Get BERT embeddings\n",
    "bert_emb1 = get_bert_embeddings(doc1)\n",
    "bert_emb2 = get_bert_embeddings(doc2)\n",
    "\n",
    "# Calculate similarity\n",
    "bert_sim = cosine_similarity(bert_emb1.reshape(1, -1), bert_emb2.reshape(1, -1))[0][0]\n",
    "\n",
    "print(f\"\\nBERT Embedding Dimensions: {len(bert_emb1)}\")\n",
    "print(f\"BERT Similarity: {bert_sim:.4f}\")\n",
    "\n",
    "# Display BERT embeddings in collapsible format\n",
    "html_bert1 = create_collapsible_vector_display(bert_emb1, \"ü§ñ Document 1 BERT Embedding\")\n",
    "html_bert2 = create_collapsible_vector_display(bert_emb2, \"ü§ñ Document 2 BERT Embedding\")\n",
    "\n",
    "display(HTML(html_bert1))\n",
    "display(HTML(html_bert2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46445577",
   "metadata": {},
   "source": [
    "## 5. Information Retrieval Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ir_metrics(text1, text2):\n",
    "    \"\"\"Calculate IR evaluation metrics.\"\"\"\n",
    "    words1 = set(preprocess_text(text1).split())\n",
    "    words2 = set(preprocess_text(text2).split())\n",
    "    \n",
    "    # Treating text2 as retrieved and text1 as relevant\n",
    "    true_positives = len(words1.intersection(words2))\n",
    "    \n",
    "    precision = true_positives / len(words2) if words2 else 0.0\n",
    "    recall = true_positives / len(words1) if words1 else 0.0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "precision, recall, f1 = calculate_ir_metrics(doc1, doc2)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Visualize metrics\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "values = [precision, recall, f1]\n",
    "colors = ['#58a6ff', '#3fb950', '#bb80ff']\n",
    "\n",
    "bars = ax.bar(metrics, values, color=colors, alpha=0.8, edgecolor='white', linewidth=2)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Information Retrieval Evaluation Metrics', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{value:.4f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfb230",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Similarity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2071228",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lsa_sim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create comprehensive comparison\u001b[39;00m\n\u001b[32m      2\u001b[39m results = {\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mJaccard Index\u001b[39m\u001b[33m'\u001b[39m: jaccard_score,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mCosine Similarity\u001b[39m\u001b[33m'\u001b[39m: cosine_sim,\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLSA Similarity\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mlsa_sim\u001b[49m,\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mBERT Similarity\u001b[39m\u001b[33m'\u001b[39m: bert_sim\n\u001b[32m      7\u001b[39m }\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create visualization\u001b[39;00m\n\u001b[32m     10\u001b[39m fig, (ax1, ax2) = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m16\u001b[39m, \u001b[32m6\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'lsa_sim' is not defined"
     ]
    }
   ],
   "source": [
    "# Create comprehensive comparison\n",
    "results = {\n",
    "    'Jaccard Index': jaccard_score,\n",
    "    'Cosine Similarity': cosine_sim,\n",
    "    'LSA Similarity': lsa_sim,\n",
    "    'BERT Similarity': bert_sim\n",
    "}\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart\n",
    "methods = list(results.keys())\n",
    "scores = list(results.values())\n",
    "colors_chart = ['#58a6ff', '#3fb950', '#bb80ff', '#f85149']\n",
    "\n",
    "bars = ax1.bar(methods, scores, color=colors_chart, alpha=0.8, edgecolor='white', linewidth=2)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_ylabel('Similarity Score', fontsize=12)\n",
    "ax1.set_title('Similarity Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=15)\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{score:.4f}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(methods), endpoint=False).tolist()\n",
    "scores_radar = scores + [scores[0]]  # Complete the circle\n",
    "angles += angles[:1]\n",
    "\n",
    "ax2 = plt.subplot(122, projection='polar')\n",
    "ax2.plot(angles, scores_radar, 'o-', linewidth=2, color='#58a6ff', markersize=8)\n",
    "ax2.fill(angles, scores_radar, alpha=0.25, color='#3fb950')\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels(methods, fontsize=10)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_title('Similarity Metrics Radar', fontsize=14, fontweight='bold', pad=20)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SIMILARITY ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for method, score in results.items():\n",
    "    print(f\"{method:20s}: {score:.4f}\")\n",
    "print(f\"\\n{'Average Similarity':20s}: {np.mean(scores):.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f5975",
   "metadata": {},
   "source": [
    "## 7. Embedding Visualization with Collapsible Summary\n",
    "\n",
    "Display all embeddings in an organized, collapsible format for detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0228016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_embedding_display():\n",
    "    \"\"\"Create a comprehensive collapsible display of all embeddings.\"\"\"\n",
    "    \n",
    "    html = \"\"\"\n",
    "    <div style=\"background: #0d1117; padding: 20px; border-radius: 12px; \n",
    "                border: 2px solid #30363d; margin: 20px 0;\">\n",
    "        <h2 style=\"color: #58a6ff; text-align: center; margin-bottom: 20px;\">\n",
    "            üìä Complete Embedding Analysis Dashboard\n",
    "        </h2>\n",
    "    \"\"\"\n",
    "    \n",
    "    # TF-IDF Section\n",
    "    html += \"\"\"\n",
    "    <div style=\"background: #161b22; border: 1px solid #30363d; \n",
    "                border-radius: 8px; padding: 16px; margin: 16px 0;\">\n",
    "        <h3 style=\"color: #3fb950; margin-bottom: 16px;\">üî§ TF-IDF Vectors</h3>\n",
    "    \"\"\"\n",
    "    html += create_collapsible_vector_display(tfidf_doc1, \"Document 1 TF-IDF\", feature_names)\n",
    "    html += create_collapsible_vector_display(tfidf_doc2, \"Document 2 TF-IDF\", feature_names)\n",
    "    html += f\"\"\"\n",
    "        <div style=\"margin-top: 16px; padding: 12px; background: #0d1117; \n",
    "                    border-radius: 6px; border-left: 4px solid #3fb950;\">\n",
    "            <strong style=\"color: #e6edf3;\">Cosine Similarity:</strong> \n",
    "            <span style=\"color: #58a6ff; font-size: 20px; font-weight: bold;\">{cosine_sim:.4f}</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # LSA Section\n",
    "    html += \"\"\"\n",
    "    <div style=\"background: #161b22; border: 1px solid #30363d; \n",
    "                border-radius: 8px; padding: 16px; margin: 16px 0;\">\n",
    "        <h3 style=\"color: #bb80ff; margin-bottom: 16px;\">üîç LSA Vectors (Dimensionality Reduced)</h3>\n",
    "    \"\"\"\n",
    "    html += create_collapsible_vector_display(lsa_doc1, \"Document 1 LSA\")\n",
    "    html += create_collapsible_vector_display(lsa_doc2, \"Document 2 LSA\")\n",
    "    html += f\"\"\"\n",
    "        <div style=\"margin-top: 16px; padding: 12px; background: #0d1117; \n",
    "                    border-radius: 6px; border-left: 4px solid #bb80ff;\">\n",
    "            <strong style=\"color: #e6edf3;\">LSA Similarity:</strong> \n",
    "            <span style=\"color: #58a6ff; font-size: 20px; font-weight: bold;\">{lsa_sim:.4f}</span>\n",
    "            <br>\n",
    "            <span style=\"color: #8b949e; font-size: 14px;\">\n",
    "                Reduced from {tfidf_matrix.shape[1]} to {n_components} dimensions\n",
    "            </span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # BERT Section\n",
    "    html += \"\"\"\n",
    "    <div style=\"background: #161b22; border: 1px solid #30363d; \n",
    "                border-radius: 8px; padding: 16px; margin: 16px 0;\">\n",
    "        <h3 style=\"color: #f85149; margin-bottom: 16px;\">ü§ñ BERT Embeddings (Contextual)</h3>\n",
    "    \"\"\"\n",
    "    html += create_collapsible_vector_display(bert_emb1, \"Document 1 BERT\")\n",
    "    html += create_collapsible_vector_display(bert_emb2, \"Document 2 BERT\")\n",
    "    html += f\"\"\"\n",
    "        <div style=\"margin-top: 16px; padding: 12px; background: #0d1117; \n",
    "                    border-radius: 6px; border-left: 4px solid #f85149;\">\n",
    "            <strong style=\"color: #e6edf3;\">BERT Similarity:</strong> \n",
    "            <span style=\"color: #58a6ff; font-size: 20px; font-weight: bold;\">{bert_sim:.4f}</span>\n",
    "            <br>\n",
    "            <span style=\"color: #8b949e; font-size: 14px;\">\n",
    "                {len(bert_emb1)}-dimensional contextual embeddings\n",
    "            </span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    html += \"</div>\"\n",
    "    return html\n",
    "\n",
    "# Display comprehensive dashboard\n",
    "display(HTML(create_comprehensive_embedding_display()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7dcaa1",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This notebook demonstrated various document similarity techniques:\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Jaccard Index**: Simple set-based measure, good for exact word matches\n",
    "- **Cosine Similarity (TF-IDF)**: Considers word importance and frequency\n",
    "- **LSA**: Captures latent semantic relationships through dimensionality reduction\n",
    "- **BERT**: State-of-the-art contextual understanding using deep learning\n",
    "\n",
    "### Use Cases:\n",
    "- **Plagiarism Detection**: High similarity scores indicate potential copying\n",
    "- **Document Clustering**: Group similar documents together\n",
    "- **Search Engines**: Rank documents by relevance to queries\n",
    "- **Recommendation Systems**: Suggest similar content to users\n",
    "\n",
    "All embeddings are displayed in collapsible format for detailed analysis while maintaining a clean interface."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
